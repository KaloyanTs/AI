{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "INTRODUCTION\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "A replacement for NumPy to use the power of GPUs\n",
    "Deep learning research platform that provides maximum flexibility and speed\n",
    "pros:\n",
    "Iinteractively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n",
    "Clean support for dynamic graphs\n",
    "Organizational backing from Facebook\n",
    "Blend of high level and low level APIs\n",
    "cons:\n",
    "Much less mature than alternatives\n",
    "Limited references / resources outside of the official documentation\n",
    "I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n",
    "Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy.\n",
    "\n",
    "Content:\n",
    "Basics of Pytorch, Linear Regression, Logistic Regression, Artificial Neural Network (ANN), Concolutional Neural Network (CNN)\n",
    "https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers/code\n",
    "Recurrent Neural Network (RNN)\n",
    "Long-Short Term Memory (LSTM)\n",
    "https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch\n",
    "\n",
    "\n",
    "Recurrent Neural Network (RNN)\n",
    "RNN is essentially repeating ANN but information get pass through from previous non-linear activation function output.\n",
    "Steps of RNN:\n",
    "Import Libraries\n",
    "Prepare Dataset\n",
    "Create RNN Model\n",
    "hidden layer dimension is 100\n",
    "number of hidden layer is 1\n",
    "Instantiate Model\n",
    "Instantiate Loss\n",
    "Cross entropy loss\n",
    "It also has softmax(logistic function) in it.\n",
    "Instantiate Optimizer\n",
    "SGD Optimizer\n",
    "Traning the Model\n",
    "Prediction\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "['train.csv', 'test.csv', 'sample_submission.csv']\n",
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Prepare Dataset\n",
    "# load data\n",
    "train = pd.read_csv(r\"../input/train.csv\",dtype = np.float32)\n",
    "\n",
    "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
    "targets_numpy = train.label.values\n",
    "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
    "\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
    "                                                                             targets_numpy,\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42) \n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# visualize one of the images in data set\n",
    "plt.imshow(features_numpy[10].reshape(28,28))\n",
    "plt.axis(\"off\")\n",
    "plt.title(str(targets_numpy[10]))\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "\n",
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 8000\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_dim = 28    # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "seq_dim = 28  \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        train  = Variable(images.view(-1, seq_dim, input_dim))\n",
    "        labels = Variable(labels )\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 250 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            if count % 500 == 0:\n",
    "                # Print Loss\n",
    "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data[0], accuracy))\n",
    "Iteration: 500  Loss: 1.4726558923721313  Accuracy: 42.726190476190474 %\n",
    "Iteration: 1000  Loss: 0.7108388543128967  Accuracy: 71.73809523809524 %\n",
    "Iteration: 1500  Loss: 0.43755194544792175  Accuracy: 85.22619047619048 %\n",
    "Iteration: 2000  Loss: 0.271086722612381  Accuracy: 90.25 %\n",
    "Iteration: 2500  Loss: 0.2235582023859024  Accuracy: 89.5 %\n",
    "Iteration: 3000  Loss: 0.09727417677640915  Accuracy: 92.66666666666667 %\n",
    "Iteration: 3500  Loss: 0.42934906482696533  Accuracy: 92.6547619047619 %\n",
    "Iteration: 4000  Loss: 0.09869173169136047  Accuracy: 94.19047619047619 %\n",
    "Iteration: 4500  Loss: 0.2372802197933197  Accuracy: 95.20238095238095 %\n",
    "Iteration: 5000  Loss: 0.10717732459306717  Accuracy: 95.19047619047619 %\n",
    "Iteration: 5500  Loss: 0.23859672248363495  Accuracy: 94.69047619047619 %\n",
    "Iteration: 6000  Loss: 0.15453924238681793  Accuracy: 96.05952380952381 %\n",
    "Iteration: 6500  Loss: 0.07914035022258759  Accuracy: 95.97619047619048 %\n",
    "Iteration: 7000  Loss: 0.12296199798583984  Accuracy: 96.27380952380952 %\n",
    "Iteration: 7500  Loss: 0.10664860904216766  Accuracy: 96.11904761904762 %\n",
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Conclusion\n",
    "In this tutorial, we learn:\n",
    "\n",
    "Basics of pytorch\n",
    "Linear regression with pytorch\n",
    "Logistic regression with pytorch\n",
    "Artificial neural network with with pytorch\n",
    "Convolutional neural network with pytorch\n",
    "https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers/code\n",
    "Recurrent neural network with pytorch\n",
    "Long-Short Term Memory (LSTM)\n",
    "https://www.kaggle.com/kanncaa1/long-short-term-memory-with-pytorch\n",
    "\n",
    "If you have any question or suggest, I will be happy to hear it"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
