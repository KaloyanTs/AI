{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3392d52-8714-4a7f-ad61-54be855e2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc354fa9-142e-4cf5-a5d3-d1f66b2376ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f72b14-c4c2-458b-bb48-71d655102bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.07276881e-278 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "W = np.random.uniform(-20, 20, size=(10, 64))\n",
    "b = np. random.uniform(-20,20, size=(10))\n",
    "\n",
    "probs = softmax(np.maximum(0, np.matmul(W,X[0])+b))\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f1acb97e-e099-4fd7-8eab-d014e7b7ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name, calculate, predecessors, derives, final=False):\n",
    "        self.name = name\n",
    "        self.calc = calculate\n",
    "        self.predecessors = list(zip(predecessors, derives))\n",
    "        self.value = None\n",
    "        self.gradient = None\n",
    "        self.final = final\n",
    "\n",
    "    def initialize(self):\n",
    "        self.value = 0\n",
    "        self.gradient = 0\n",
    "        \n",
    "def forward(nodes):\n",
    "    order = topoSort(nodes)\n",
    "    for node in order:\n",
    "        # print(\"calculating \", node.name)\n",
    "        # print([p[0].value.shape for p in node.predecessors])\n",
    "        node.value = node.calc(*[p[0].value for p in node.predecessors])\n",
    "    # print (\"final forward \",order[-1].value.shape, order[-1].value)\n",
    "\n",
    "def topoSortRec(node, result):\n",
    "    if node in result:\n",
    "        return\n",
    "    for parent in node.predecessors:\n",
    "        topoSortRec(parent[0], result)\n",
    "    result.append(node)\n",
    "\n",
    "def topoSort(nodes):\n",
    "    result = []\n",
    "    for node in nodes:\n",
    "        topoSortRec(node, result)\n",
    "    return result\n",
    "\n",
    "def backpropagation(nodes):\n",
    "    order = topoSort(nodes)\n",
    "    for v in order:\n",
    "        v.gradient = np.zeros_like(v.value)\n",
    "        if v.final:\n",
    "            v.gradient = 1\n",
    "    for v in order[::-1]:\n",
    "        values = [node[0].value for node in v.predecessors]\n",
    "        for node, derive in v.predecessors:\n",
    "            node.gradient += v.gradient * derive(*values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "37fef7ec-af9f-4a81-a7cd-51872b0519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input node\n",
    "input_node = Node(\"X\", lambda: X, [], [])\n",
    "\n",
    "# Define the weights and biases for the hidden layer\n",
    "W_hidden = Node(\"W_hidden\", lambda: np.random.uniform(-1, 1, size=(64, 32)), [], [])\n",
    "b_hidden = Node(\"bias_hidden\", lambda: np.random.uniform(-1, 1, size=(32,)), [], [])\n",
    "\n",
    "# Define the hidden layer node\n",
    "hidden_layer = Node(\"hidden_layer\", lambda x, W, b: np.maximum(0, np.matmul(x, W) + b), [input_node, W_hidden, b_hidden], [lambda x, W, b: None, lambda W, x, b: np.heaviside(np.matmul(x, W) + b, 0), lambda W, x, b: np.heaviside(np.matmul(x, W) + b, 0)])\n",
    "\n",
    "# Define the weights and biases for the output layer\n",
    "W_output = Node(\"W_output\", lambda: np.random.uniform(-1, 1, size=(32, 10)), [], [])\n",
    "b_output = Node(\"bias_output\", lambda: np.random.uniform(-1, 1, size=(10,)), [], [])\n",
    "\n",
    "# Define the output layer node\n",
    "output_layer = Node(\"output_layer\", lambda h, W, b: softmax(np.matmul(h, W) + b, axis=1), [hidden_layer, W_output, b_output], [lambda h, W, b: softmax(np.matmul(h, W) + b, axis=1), lambda W, h, b: softmax(np.matmul(h, W) + b, axis=1), lambda W, h, b: softmax(np.matmul(h, W) + b, axis=1)])\n",
    "\n",
    "y_node = Node(\"y_labels\", lambda: y, [], [])\n",
    "\n",
    "# Define the cross-entropy loss node\n",
    "cross_entropy_loss = Node(\n",
    "    \"cross_entropy_loss\",\n",
    "    lambda output, y: -np.sum(np.log(output[np.arange(len(y)), y])) / len(y),\n",
    "    [output_layer, y_node],\n",
    "    [lambda output, y: -1 / len(y) * (1 / output[np.arange(len(y)), y]), lambda output, y: None],\n",
    "    True\n",
    ")\n",
    "\n",
    "# Initialize and run the forward pass\n",
    "nodes = [input_node, W_hidden, b_hidden, hidden_layer, W_output, b_output, output_layer, cross_entropy_loss, y_node]\n",
    "for node in nodes:\n",
    "    node.initialize()\n",
    "\n",
    "# forward(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f736f2bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1797,10) (1797,) (1797,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m     best_b_output \u001b[38;5;241m=\u001b[39m b_output\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m backpropagation(nodes)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Gradient descent update\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "Cell \u001b[1;32mIn[142], line 44\u001b[0m, in \u001b[0;36mbackpropagation\u001b[1;34m(nodes)\u001b[0m\n\u001b[0;32m     42\u001b[0m values \u001b[38;5;241m=\u001b[39m [node[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpredecessors]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node, derive \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpredecessors:\n\u001b[1;32m---> 44\u001b[0m     node\u001b[38;5;241m.\u001b[39mgradient \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mgradient \u001b[38;5;241m*\u001b[39m derive(\u001b[38;5;241m*\u001b[39mvalues)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1797,10) (1797,) (1797,10) "
     ]
    }
   ],
   "source": [
    "# forward(nodes)\n",
    "# best_loss = float('inf')\n",
    "# for i in range(200000):\n",
    "#     forward(nodes)\n",
    "#     loss = cross_entropy_loss.value\n",
    "#     if loss < 45:\n",
    "#         print(i, \"-> \", loss)\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_W_hidden = W_hidden.value.copy()\n",
    "#         best_b_hidden = b_hidden.value.copy()\n",
    "#         best_W_output = W_output.value.copy()\n",
    "#         best_b_output = b_output.value.copy()\n",
    "\n",
    "# print(\"Best loss:\", best_loss)\n",
    "\n",
    "# # Define a new neural network with the best parameters\n",
    "# new_input_node = Node(\"X\", lambda: X, [], [])\n",
    "\n",
    "# new_W_hidden = Node(\"W_hidden\", lambda: best_W_hidden, [], [])\n",
    "# new_b_hidden = Node(\"bias_hidden\", lambda: best_b_hidden, [], [])\n",
    "\n",
    "# new_hidden_layer = Node(\"hidden_layer\", lambda x, W, b: np.maximum(0, np.matmul(x, W) + b), [new_input_node, new_W_hidden, new_b_hidden], [lambda W, x, b: None, lambda W, x, b: np.heaviside(np.matmul(x, W) + b, 0), lambda W, x, b: np.heaviside(np.matmul(x, W) + b, 0)])\n",
    "\n",
    "# new_W_output = Node(\"W_output\", lambda: best_W_output, [], [])\n",
    "# new_b_output = Node(\"bias_output\", lambda: best_b_output, [], [])\n",
    "\n",
    "# new_output_layer = Node(\"output_layer\", lambda h, W, b: softmax(np.matmul(h, W) + b, axis=1), [new_hidden_layer, new_W_output, new_b_output], [lambda W, h, b: softmax(np.matmul(h, W) + b, axis=1), lambda W, h, b: softmax(np.matmul(h, W) + b, axis=1), lambda W, h, b: softmax(np.matmul(h, W) + b, axis=1)])\n",
    "\n",
    "# new_y_node = Node(\"y_labels\", lambda: y, [], [])\n",
    "\n",
    "# new_cross_entropy_loss = Node(\n",
    "#     \"cross_entropy_loss\",\n",
    "#     lambda output, y: -np.sum(np.log(output[np.arange(len(y)), y])) / len(y),\n",
    "#     [new_output_layer, new_y_node],\n",
    "#     [lambda output, y: -1 / len(y) * (1 / output[np.arange(len(y)), y]), lambda output, y: None],\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# # Initialize and run the forward pass for the new network\n",
    "# new_nodes = [new_input_node, new_W_hidden, new_b_hidden, new_hidden_layer, new_W_output, new_b_output, new_output_layer, new_cross_entropy_loss, new_y_node]\n",
    "# for node in new_nodes:\n",
    "#     node.initialize()\n",
    "\n",
    "# forward(new_nodes)\n",
    "# new_final_loss = new_cross_entropy_loss.value\n",
    "# print(\"New final loss with best parameters:\", new_final_loss)\n",
    "\n",
    "# new_correct_predictions = np.sum(np.argmax(new_output_layer.value, axis=1) == y)\n",
    "# new_final_accuracy = new_correct_predictions / len(y) * 100\n",
    "# print(\"New final Accuracy: {:.2f}%\".format(new_final_accuracy))\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(200000):\n",
    "    forward(nodes)\n",
    "    loss = cross_entropy_loss.value\n",
    "    if loss < 45:\n",
    "        print(i, \"-> \", loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_W_hidden = W_hidden.value.copy()\n",
    "        best_b_hidden = b_hidden.value.copy()\n",
    "        best_W_output = W_output.value.copy()\n",
    "        best_b_output = b_output.value.copy()\n",
    "\n",
    "    # Backpropagation\n",
    "    backpropagation(nodes)\n",
    "\n",
    "    # Gradient descent update\n",
    "    for node in nodes:\n",
    "        if node.name in [\"W_hidden\", \"bias_hidden\", \"W_output\", \"bias_output\"]:\n",
    "            node.value -= learning_rate * node.gradient\n",
    "\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
