{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0163a505-5e40-4639-9dba-44713317c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def crossEntropyS(X, Y, w, b):\n",
    "\t# X.shape = (S,N) Y.shape = (S), W.shape = (N) \n",
    "    v = sigmoid(np.dot(X,w)+b)\n",
    "    p = (1-Y) + (2*Y-1)*v\n",
    "    ce = -np.mean(np.log(p))\n",
    "    return ce\n",
    "\n",
    "def gradCrossEntropyS(X,Y,w,b):\n",
    "    g = Y - sigmoid(np.dot(X,w)+b)\n",
    "    db = -np.mean(g)\n",
    "    dw = -np.mean( g[:,np.newaxis] * X,axis=0)\n",
    "    return dw, db   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "56d422b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition: 49000/50000\n",
      "error: 0.39270570526688353\n",
      "[0.03 0.71 0.69 0.57]\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# helped by:\n",
    "# https://www.kaggle.com/code/anubhavgoyal10/one-hidden-layer-neural-network-from-scratch\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# hidden_layers = int(input(\"Брой неврони от скрития слой\"))\n",
    "# learn_repeat = int(input(\"Брой повторения на обучаването\"))\n",
    "k = 10\n",
    "learn_repeat = 50000\n",
    "learning_rate = 0.5\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_and = np.array([0,0,0,1])\n",
    "y_or = np.array([0,1,1,1])\n",
    "y_xor = np.array([0,1,1,0])\n",
    "\n",
    "y = y_xor\n",
    "\n",
    "# -------------------TEST-------------------\n",
    "# from sklearn.datasets import make_gaussian_quantiles\n",
    "# N = 500\n",
    "# X, y = make_gaussian_quantiles(mean=None, cov=0.5, n_samples = N, \n",
    "#                                              n_features = 2, n_classes=2, shuffle = True, \n",
    "#                                              random_state = 42)\n",
    "# -------------------TEST-------------------\n",
    "\n",
    "\n",
    "\n",
    "Wh = np.random.uniform(-1, 1, (X.shape[1], k))\n",
    "bh = np.random.uniform(-1, 1, (k,))\n",
    "\n",
    "Wo = np.random.uniform(-1, 1, (k, ))\n",
    "bo = np.random.uniform(-1, 1)\n",
    "\n",
    "for i in range(learn_repeat):\n",
    "    if i%1000 == 0:\n",
    "        print(f\"\\rRepetition: {i}/{learn_repeat}\", end='', flush=True)\n",
    "    q = np.dot(X, Wh) + bh\n",
    "    # r = np.tanh(q)\n",
    "    r = sigmoid(q)\n",
    "    s = np.dot(r, Wo) + bo\n",
    "    sc = sigmoid(s)\n",
    "    p = (1-y) + (2*y-1)*sc\n",
    "    error = -np.mean(np.log(p))\n",
    "    # print(p)\n",
    "    # print(error)\n",
    "    \n",
    "    dWo, dbo = gradCrossEntropyS(r,y,Wo,bo)\n",
    "    ds = -np.mean(y - sc)\n",
    "    dt = ds\n",
    "    dr = np.outer(np.ones(k), ds).T * np.outer(np.ones(X.shape[0]), Wo)\n",
    "    # dq = dr * (1 - np.power(np.tanh(q),2))\n",
    "    dq = dr * sigmoid_derivative(q)\n",
    "    dbh = np.mean(dq, axis=0)\n",
    "    dWh = X.T @ dq # should there be a division by k?\n",
    "    \n",
    "    Wh = Wh - learning_rate * dWh\n",
    "    Wo = Wo - learning_rate * dWo\n",
    "    bh = bh - learning_rate * dbh\n",
    "    bo = bo - learning_rate * dbo\n",
    "# print(sc)\n",
    "print(\"\\nerror: \" + str(error))\n",
    "print(sc.round(decimals=2))\n",
    "print(np.sum(p > 0.5))\n",
    "print(X.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
